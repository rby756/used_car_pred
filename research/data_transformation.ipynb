{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.carPricePrediction import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass\n",
    "# from pathlib import Path\n",
    "\n",
    "# @dataclass(frozen=True)\n",
    "# class DataTransformationConfig:\n",
    "#     root_dir: Path\n",
    "#     data_path: Path\n",
    "#     preprocessor_name: str\n",
    "#     target_column: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.carPricePrediction.constants import *\n",
    "from src.carPricePrediction.utils.common import read_yaml, create_directories\n",
    "# from src.carPricePrediction.entity.config_entity import (DataIngestionConfig, \n",
    "#                                                 DataValidationConfig,\n",
    "#                                                 DataTransformationConfig)\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root]) \n",
    "   \n",
    "   \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        schema = self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            preprocessor_name=config.preprocessor_name,\n",
    "            target_column=schema.name\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer for Target Encoding\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TargetEncodingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols=None):\n",
    "        self.cols = cols\n",
    "        self.encoder = TargetEncoder(cols=self.cols)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.encoder.fit(X[self.cols], y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X[self.cols] = self.encoder.transform(X[self.cols])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        \"\"\"\n",
    "        Initialize the DataTransformation class with a given configuration.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    def _drop_irrelevant_columns(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Drop irrelevant columns from the data.\n",
    "\n",
    "        Parameters:\n",
    "        data (pd.DataFrame): The input data.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The data without the dropped columns.\n",
    "        \"\"\"\n",
    "\n",
    "        data=data.drop(columns=['car_name','registration_year'])\n",
    "        print(data.columns)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _handle_missing_values(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Drop rows with missing target values and handle other missing values as needed.\n",
    "\n",
    "        Parameters:\n",
    "        data (pd.DataFrame): The raw input data.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The data with missing target values removed.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"self.config.target_column : \", self.config.target_column)\n",
    "        data = data.dropna(subset=[self.config.target_column])\n",
    "\n",
    "        # Additional handling for missing values in features can be added here if needed\n",
    "        return data\n",
    "\n",
    "    def _separate_features_and_target(self, data: pd.DataFrame) -> tuple:\n",
    "        \"\"\"\n",
    "        Separate features and target variable from the data.\n",
    "\n",
    "        Parameters:\n",
    "        data (pd.DataFrame): The input data.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Features (X) and target (y).\n",
    "        \"\"\"\n",
    "        X = data.drop(columns=self.config.target_column)\n",
    "        y = data[self.config.target_column]\n",
    "        return X, y\n",
    "\n",
    "    def _select_columns_by_type(self, X: pd.DataFrame) -> tuple:\n",
    "        \"\"\"\n",
    "        Select numerical and categorical columns from the feature data.\n",
    "\n",
    "        Parameters:\n",
    "        X (pd.DataFrame): The feature data.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Lists of numerical and categorical column names.\n",
    "        \"\"\"\n",
    "        num_cols = ['manufacturing_year', 'seats', 'kms_driven',\n",
    "       'mileage(kmpl)', 'engine(cc)', 'torque(Nm)']\n",
    "\n",
    "        cat_cols = ['insurance_validity', 'fuel_type', 'ownsership', 'transmission']\n",
    "\n",
    "        return num_cols, cat_cols\n",
    "\n",
    "    def _create_transformer(self, num_cols: list, cat_cols: list) -> ColumnTransformer:\n",
    "        \"\"\"\n",
    "        Create a column transformer for preprocessing.\n",
    "\n",
    "        Parameters:\n",
    "        num_cols (list): List of numerical column names.\n",
    "        cat_cols (list): List of categorical column names.\n",
    "\n",
    "        Returns:\n",
    "        ColumnTransformer: The column transformer.\n",
    "        \"\"\"\n",
    "        num_pipeline = Pipeline(steps=[\n",
    "            ('imputer', KNNImputer(n_neighbors=3)),  # Using KNNImputer\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "\n",
    "        cat_pipeline = ColumnTransformer(\n",
    "        transformers=[\n",
    "            # Label Encoding for insurance_validity\n",
    "            ('insurance_validity', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), ['insurance_validity']),\n",
    "\n",
    "            # One-Hot Encoding for fuel_type and transmission\n",
    "            ('fuel_type', OneHotEncoder(handle_unknown='ignore'), ['fuel_type']),\n",
    "            ('transmission', OneHotEncoder(handle_unknown='ignore'), ['transmission']),\n",
    "\n",
    "            # Ordinal Encoding for ownership\n",
    "            ('ownsership', OrdinalEncoder(categories=[['First Owner', 'Second Owner', 'Third Owner', 'Fourth Owner','Fifth Owner']]), ['ownsership'])\n",
    "        ],\n",
    "        \n",
    "        remainder='passthrough'  # Pass the numeric features through without transformation\n",
    ")\n",
    "\n",
    "        transformer = ColumnTransformer(transformers=[\n",
    "            ('num_pipeline', num_pipeline, num_cols),\n",
    "            ('cat_pipeline', cat_pipeline, cat_cols),\n",
    "        ], remainder='drop', n_jobs=-1)\n",
    "\n",
    "        return transformer\n",
    "\n",
    "    def _save_transformer(self, transformer: ColumnTransformer) -> None:\n",
    "        \"\"\"\n",
    "        Save the fitted transformer to a file.\n",
    "\n",
    "        Parameters:\n",
    "        transformer (ColumnTransformer): The fitted column transformer.\n",
    "        \"\"\"\n",
    "        joblib.dump(transformer, os.path.join(self.config.root_dir, self.config.preprocessor_name))\n",
    "\n",
    "\n",
    "    def _apply_target_encoding(self, X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:\n",
    "\n",
    "        \"\"\"\n",
    "        Apply target encoding to specified categorical columns.\n",
    "\n",
    "        Parameters:\n",
    "        X (pd.DataFrame): The feature data.\n",
    "        y (pd.Series): The target variable.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The feature data with target-encoded columns.\n",
    "        \"\"\"\n",
    "        \n",
    "        target_encoder = TargetEncodingTransformer(cols=['short_carname'])\n",
    "        target_encoder.fit(X, y)\n",
    "        return target_encoder\n",
    "\n",
    "    def preprocess_data(self, X : pd.DataFrame,y : pd.DataFrame, fit: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform the complete preprocessing pipeline on the data.\n",
    "\n",
    "        This includes:\n",
    "        - Dropping irrelevant columns\n",
    "        - Handling missing values (dropping rows with missing target)\n",
    "        - Separating features and target\n",
    "        - Applying target encoding\n",
    "        - Selecting numerical and categorical columns\n",
    "        - Creating and saving the transformer\n",
    "        - Transforming the feature data\n",
    "\n",
    "        Parameters:\n",
    "        data (pd.DataFrame): The raw input data.\n",
    "        fit (bool): Whether to fit the transformers or just transform using pre-fitted transformers.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Transformed feature data (X_transformed) and target variable (y).\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        target_encoder=self._load_target_encoder()\n",
    "        X=target_encoder.transform(X)\n",
    "        transformer = joblib.load(os.path.join(self.config.root_dir, self.config.preprocessor_name))\n",
    "        X_transformed = transformer.transform(X)\n",
    "\n",
    "\n",
    "        return X_transformed, y\n",
    "\n",
    "    def _save_target_encoder(self, encoder: TargetEncodingTransformer) -> None:\n",
    "        joblib.dump(encoder, os.path.join(self.config.root_dir, 'target_encoder.pkl'))\n",
    "\n",
    "    def _load_target_encoder(self) -> TargetEncodingTransformer:\n",
    "        return joblib.load(os.path.join(self.config.root_dir, 'target_encoder.pkl'))\n",
    "\n",
    "    def train_test_splitting(self) -> None:\n",
    "        \"\"\"\n",
    "        Load data, preprocess it, and split into training and test sets.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data = pd.read_csv(self.config.data_path)\n",
    "\n",
    "\n",
    "            data = self._drop_irrelevant_columns(data)\n",
    "            data = self._handle_missing_values(data)\n",
    "\n",
    "            \n",
    "            X, y = self._separate_features_and_target(data)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "            num_cols, cat_cols = self._select_columns_by_type(X)\n",
    "            transformer = self._create_transformer(num_cols, cat_cols)\n",
    "            transformer.fit(X_train,y_train)\n",
    "            self._save_transformer(transformer)\n",
    "\n",
    "            target_encoder = self._apply_target_encoding(X_train,y_train)\n",
    "            self._save_target_encoder(target_encoder)\n",
    "            \n",
    "\n",
    "            X_train,y_train = self.preprocess_data(X_train,y_train)\n",
    "            X_test,y_test = self.preprocess_data(X_test,y_test)\n",
    "\n",
    "\n",
    "\n",
    "            X_train = pd.DataFrame(X_train).reset_index()\n",
    "            X_test = pd.DataFrame(X_test).reset_index()\n",
    "\n",
    "            y_train = pd.DataFrame(y_train).reset_index()\n",
    "            y_test = pd.DataFrame(y_test).reset_index()\n",
    "\n",
    "            print(\"checking null values \",X_train.isnull().sum())\n",
    "\n",
    "            print(\"checking length of \",len(X_train),len(y_train))\n",
    "\n",
    "            train_processed = pd.concat([X_train, y_train], axis=1)\n",
    "            test_processed = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "            print(\"checking null values \",train_processed.isnull().sum())\n",
    "\n",
    "            train_processed.to_csv(os.path.join(self.config.root_dir, \"train.csv\"), index=False)\n",
    "            test_processed.to_csv(os.path.join(self.config.root_dir, \"test.csv\"), index=False)\n",
    "\n",
    "            logger.info(\"Data split into training and test sets\")\n",
    "            logger.info(f\"Shape of preprocessed training data: {train_processed.shape}\")\n",
    "            logger.info(f\"Shape of preprocessed test data: {test_processed.shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"An error occurred during train-test splitting\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-07 19:41:22,496: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-07-07 19:41:22,502: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-07-07 19:41:22,516: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-07-07 19:41:22,521: INFO: common: created directory at: artifacts]\n",
      "1\n",
      "[2024-07-07 19:41:22,527: INFO: common: created directory at: artifacts/data_transformation]\n",
      "2\n",
      "3\n",
      "Index(['insurance_validity', 'fuel_type', 'ownsership', 'transmission',\n",
      "       'manufacturing_year', 'seats', 'kms_driven', 'mileage(kmpl)',\n",
      "       'engine(cc)', 'torque(Nm)', 'price(in lakhs)', 'short_carname'],\n",
      "      dtype='object')\n",
      "self.config.target_column :  price(in lakhs)\n",
      "checking null values  index    0\n",
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "5        0\n",
      "6        0\n",
      "7        0\n",
      "8        0\n",
      "9        0\n",
      "10       0\n",
      "11       0\n",
      "12       0\n",
      "dtype: int64\n",
      "checking length of  813 813\n",
      "checking null values  index              0\n",
      "0                  0\n",
      "1                  0\n",
      "2                  0\n",
      "3                  0\n",
      "4                  0\n",
      "5                  0\n",
      "6                  0\n",
      "7                  0\n",
      "8                  0\n",
      "9                  0\n",
      "10                 0\n",
      "11                 0\n",
      "12                 0\n",
      "index              0\n",
      "price(in lakhs)    0\n",
      "dtype: int64\n",
      "[2024-07-07 19:41:22,656: INFO: 2589047956: Data split into training and test sets]\n",
      "[2024-07-07 19:41:22,656: INFO: 2589047956: Shape of preprocessed training data: (813, 16)]\n",
      "[2024-07-07 19:41:22,657: INFO: 2589047956: Shape of preprocessed test data: (272, 16)]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    print(\"1\")\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    print(\"2\")\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    print(\"3\")\n",
    "    data_transformation.train_test_splitting()\n",
    "    print(\"4\")\n",
    "except Exception as e:\n",
    "    print(\"5\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
